{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4913a7e",
   "metadata": {},
   "source": [
    "# Problema 4: Geração de Texto com RNN, LSTM e GRU\n",
    "\n",
    "**Objetivo:** Comparar o desempenho de três arquiteturas de redes neurais recorrentes (`SimpleRNN`, `LSTM` e `GRU`) em uma tarefa de geração de texto.\n",
    "\n",
    "**Abordagem:**\n",
    "1.  Carregar e pré-processar o texto de três livros da série Harry Potter.\n",
    "2.  Utilizar um **Tokenizador** para criar um vocabulário de palavras.\n",
    "3.  Treinar cada modelo para prever a próxima palavra em uma sequência.\n",
    "4.  Gerar texto com cada modelo para comparar visualmente a coerência e qualidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290a7c5",
   "metadata": {},
   "source": [
    "### 1. Importação das Bibliotecas\n",
    "\n",
    "Vamos importar todas as bibliotecas necessárias para o projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41dcf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 09:42:21.092657: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e30d31",
   "metadata": {},
   "source": [
    "### 2. Carregamento e Pré-processamento dos Dados\n",
    "\n",
    "Carregamos os textos, juntamos em um único corpus e usamos o `Tokenizer` para criar um vocabulário de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953ed210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 16633 palavras\n"
     ]
    }
   ],
   "source": [
    "# Carregar e concatenar os textos\n",
    "text = \"\"\n",
    "for i in range(1, 4):\n",
    "    filepath = os.path.join('dataset', f'harry_potter_{i}.txt')\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text += f.read()\n",
    "\n",
    "# --- Tokenização por Palavra ---\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "word_to_int = tokenizer.word_index\n",
    "int_to_word = {i: w for w, i in word_to_int.items()}\n",
    "vocab_size = len(word_to_int) + 1\n",
    "print(f\"Tamanho do vocabulário: {vocab_size} palavras\")\n",
    "\n",
    "# Converter todo o texto para uma sequência de inteiros\n",
    "full_sequence = tokenizer.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377a74f",
   "metadata": {},
   "source": [
    "### 3. Criação das Sequências de Treino\n",
    "\n",
    "Transformamos a longa sequência de palavras em pares de `(entrada, saída)` para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f6b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de sequências de treino: 291509\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50  # Usaremos 50 palavras para prever a 51ª\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(seq_length, len(full_sequence)):\n",
    "    in_seq = full_sequence[i-seq_length:i]\n",
    "    out_word = full_sequence[i]\n",
    "    X_data.append(in_seq)\n",
    "    y_data.append(out_word)\n",
    "\n",
    "n_patterns = len(X_data)\n",
    "print(f\"Total de sequências de treino: {n_patterns}\")\n",
    "\n",
    "# Preparar os dados para a rede neural\n",
    "X = np.array(X_data)\n",
    "y = np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4584995",
   "metadata": {},
   "source": [
    "### 4. Definição do Modelo e Funções de Apoio\n",
    "\n",
    "Criamos funções para construir os modelos e gerar texto. Também configuramos os `callbacks` para um treinamento eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff0b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(recurrent_layer, vocab_size, seq_length):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=100, input_length=seq_length),\n",
    "        recurrent_layer(128, return_sequences=True),\n",
    "        recurrent_layer(128),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_text(model, tokenizer, seed_text, num_words_to_gen=40, temperature=0.7):\n",
    "    full_generated_text = seed_text.lower()\n",
    "    print(f\"Semente: \\\"{seed_text}\\\" | Temperatura: {temperature}\")\n",
    "    print(\"Texto gerado:\")\n",
    "    print(\"------------------\")\n",
    "    print(seed_text, end=' ')\n",
    "    \n",
    "    for _ in range(num_words_to_gen):\n",
    "        token_list = tokenizer.texts_to_sequences([full_generated_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
    "        \n",
    "        # Previsão e aplicação da temperatura\n",
    "        prediction = model.predict(token_list, verbose=0)[0]\n",
    "        prediction = np.log(prediction + 1e-7) / temperature # Adicionado 1e-7 para evitar log(0)\n",
    "        \n",
    "        # Amostragem da próxima palavra\n",
    "        index = tf.random.categorical(prediction[np.newaxis, :], num_samples=1)[0, 0].numpy()\n",
    "        \n",
    "        output_word = int_to_word.get(index, \"<unk>\")\n",
    "        full_generated_text += \" \" + output_word\n",
    "        print(output_word, end=' ')\n",
    "    print(\"\\n------------------\\n\")\n",
    "\n",
    "# Parâmetros de treino\n",
    "epochs = 170\n",
    "seed = \"harry potter olhou para o castelo e\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1749af",
   "metadata": {},
   "source": [
    "### 5. Treinamento e Avaliação dos Modelos\n",
    "\n",
    "Treinamos cada modelo sequencialmente, gerando o texto e limpando a memória da GPU após cada um para evitar sobrecarga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8239bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Treinando o Modelo SimpleRNN ###\n",
      "Epoch 1/170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardopn/Documents/Git/UFSM/rnn-lstm-presentation/.venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760272944.019274    7468 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5465 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:07:00.0, compute capability: 8.9\n",
      "2025-10-12 09:42:25.645322: I external/local_xla/xla/service/service.cc:163] XLA service 0x7effb8003850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-12 09:42:25.645336: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2025-10-12 09:42:25.727899: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-12 09:42:32.078518: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 90300\n",
      "2025-10-12 09:42:34.340704: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:34.340740: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:34.340755: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:34.340774: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:34.340790: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:34.340807: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:34.340823: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:42:50.035102: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_61', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:42:50.592446: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_60', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:42:51.462684: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_65', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "2025-10-12 09:42:51.497745: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_65', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  21/2050\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.0019 - loss: 9.6919       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760272973.739846    7562 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2046/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0451 - loss: 7.1998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 09:43:10.028670: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.028694: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.028704: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.028718: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.028731: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.028743: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.028755: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:10.289631: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_61', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:10.468601: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_60', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:10.680508: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_61', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:10.701500: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_61', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:10.989704: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_60', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:11.053027: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_60', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:11.489513: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1178', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:11.665263: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:11.799409: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:11.929138: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0451 - loss: 7.1990"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 09:43:15.142860: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:15.142883: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:15.142893: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-12 09:43:15.719770: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_24', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:15.790555: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_24', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:15.829932: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:15.922434: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:15.937501: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-12 09:43:16.049755: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_24', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 11ms/step - accuracy: 0.0563 - loss: 6.7943 - val_accuracy: 0.0859 - val_loss: 6.1942\n",
      "Epoch 2/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1045 - loss: 5.9353 - val_accuracy: 0.1204 - val_loss: 5.7133\n",
      "Epoch 3/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1286 - loss: 5.5031 - val_accuracy: 0.1373 - val_loss: 5.5690\n",
      "Epoch 4/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1444 - loss: 5.2222 - val_accuracy: 0.1450 - val_loss: 5.4975\n",
      "Epoch 5/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1549 - loss: 5.0090 - val_accuracy: 0.1502 - val_loss: 5.4734\n",
      "Epoch 6/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1649 - loss: 4.8296 - val_accuracy: 0.1519 - val_loss: 5.4834\n",
      "Epoch 7/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1739 - loss: 4.6656 - val_accuracy: 0.1517 - val_loss: 5.4922\n",
      "Epoch 8/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1796 - loss: 4.5494 - val_accuracy: 0.1494 - val_loss: 5.5200\n",
      "Epoch 9/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1894 - loss: 4.4033 - val_accuracy: 0.1496 - val_loss: 5.5385\n",
      "Epoch 10/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.1993 - loss: 4.2740 - val_accuracy: 0.1486 - val_loss: 5.5696\n",
      "Epoch 11/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2102 - loss: 4.1476 - val_accuracy: 0.1481 - val_loss: 5.6045\n",
      "Epoch 12/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2197 - loss: 4.0449 - val_accuracy: 0.1502 - val_loss: 5.6358\n",
      "Epoch 13/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2314 - loss: 3.9403 - val_accuracy: 0.1474 - val_loss: 5.6712\n",
      "Epoch 14/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2445 - loss: 3.8353 - val_accuracy: 0.1469 - val_loss: 5.7183\n",
      "Epoch 15/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2555 - loss: 3.7470 - val_accuracy: 0.1470 - val_loss: 5.7635\n",
      "Epoch 16/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2670 - loss: 3.6578 - val_accuracy: 0.1464 - val_loss: 5.7855\n",
      "Epoch 17/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2766 - loss: 3.5868 - val_accuracy: 0.1435 - val_loss: 5.8361\n",
      "Epoch 18/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2854 - loss: 3.5307 - val_accuracy: 0.1410 - val_loss: 5.8791\n",
      "Epoch 19/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2953 - loss: 3.4589 - val_accuracy: 0.1415 - val_loss: 5.9121\n",
      "Epoch 20/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.2882 - loss: 3.5217 - val_accuracy: 0.1380 - val_loss: 5.9378\n",
      "Epoch 21/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3089 - loss: 3.3626 - val_accuracy: 0.1377 - val_loss: 5.9704\n",
      "Epoch 22/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3230 - loss: 3.2733 - val_accuracy: 0.1401 - val_loss: 6.0150\n",
      "Epoch 23/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3253 - loss: 3.2591 - val_accuracy: 0.1345 - val_loss: 6.0699\n",
      "Epoch 24/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3057 - loss: 3.3876 - val_accuracy: 0.1352 - val_loss: 6.0678\n",
      "Epoch 25/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3305 - loss: 3.2104 - val_accuracy: 0.1373 - val_loss: 6.1010\n",
      "Epoch 26/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3508 - loss: 3.0828 - val_accuracy: 0.1335 - val_loss: 6.1445\n",
      "Epoch 27/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3596 - loss: 3.0351 - val_accuracy: 0.1344 - val_loss: 6.1812\n",
      "Epoch 28/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3623 - loss: 3.0128 - val_accuracy: 0.1368 - val_loss: 6.2050\n",
      "Epoch 29/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3577 - loss: 3.0360 - val_accuracy: 0.1319 - val_loss: 6.2469\n",
      "Epoch 30/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3702 - loss: 2.9548 - val_accuracy: 0.1299 - val_loss: 6.2785\n",
      "Epoch 31/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3822 - loss: 2.8930 - val_accuracy: 0.1315 - val_loss: 6.3214\n",
      "Epoch 32/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3752 - loss: 2.9221 - val_accuracy: 0.1283 - val_loss: 6.3456\n",
      "Epoch 33/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3769 - loss: 2.9097 - val_accuracy: 0.1307 - val_loss: 6.3582\n",
      "Epoch 34/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3935 - loss: 2.8148 - val_accuracy: 0.1273 - val_loss: 6.3965\n",
      "Epoch 35/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3819 - loss: 2.8808 - val_accuracy: 0.1249 - val_loss: 6.4495\n",
      "Epoch 36/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3996 - loss: 2.7782 - val_accuracy: 0.1292 - val_loss: 6.4606\n",
      "Epoch 37/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3642 - loss: 3.0129 - val_accuracy: 0.1288 - val_loss: 6.4295\n",
      "Epoch 38/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3795 - loss: 2.8743 - val_accuracy: 0.1281 - val_loss: 6.4412\n",
      "Epoch 39/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4159 - loss: 2.6833 - val_accuracy: 0.1277 - val_loss: 6.4977\n",
      "Epoch 40/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4183 - loss: 2.6664 - val_accuracy: 0.1287 - val_loss: 6.5345\n",
      "Epoch 41/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4193 - loss: 2.6518 - val_accuracy: 0.1247 - val_loss: 6.5651\n",
      "Epoch 42/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4154 - loss: 2.6716 - val_accuracy: 0.1267 - val_loss: 6.5712\n",
      "Epoch 43/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4196 - loss: 2.6501 - val_accuracy: 0.1226 - val_loss: 6.6081\n",
      "Epoch 44/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4304 - loss: 2.5911 - val_accuracy: 0.1237 - val_loss: 6.6388\n",
      "Epoch 45/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4115 - loss: 2.6842 - val_accuracy: 0.1209 - val_loss: 6.6489\n",
      "Epoch 46/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4363 - loss: 2.5541 - val_accuracy: 0.1236 - val_loss: 6.6873\n",
      "Epoch 47/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4211 - loss: 2.6254 - val_accuracy: 0.1240 - val_loss: 6.6943\n",
      "Epoch 48/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4420 - loss: 2.5221 - val_accuracy: 0.1216 - val_loss: 6.7239\n",
      "Epoch 49/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4360 - loss: 2.5444 - val_accuracy: 0.1195 - val_loss: 6.7434\n",
      "Epoch 50/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4466 - loss: 2.4933 - val_accuracy: 0.1206 - val_loss: 6.7938\n",
      "Epoch 51/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4339 - loss: 2.5483 - val_accuracy: 0.1187 - val_loss: 6.7669\n",
      "Epoch 52/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4392 - loss: 2.5206 - val_accuracy: 0.1183 - val_loss: 6.7953\n",
      "Epoch 53/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4535 - loss: 2.4501 - val_accuracy: 0.1208 - val_loss: 6.8229\n",
      "Epoch 54/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4405 - loss: 2.5125 - val_accuracy: 0.1216 - val_loss: 6.8123\n",
      "Epoch 55/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4555 - loss: 2.4334 - val_accuracy: 0.1184 - val_loss: 6.8481\n",
      "Epoch 56/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4515 - loss: 2.4527 - val_accuracy: 0.1183 - val_loss: 6.8696\n",
      "Epoch 57/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4554 - loss: 2.4299 - val_accuracy: 0.1186 - val_loss: 6.8878\n",
      "Epoch 58/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4455 - loss: 2.4784 - val_accuracy: 0.1215 - val_loss: 6.8747\n",
      "Epoch 59/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4663 - loss: 2.3778 - val_accuracy: 0.1144 - val_loss: 6.9095\n",
      "Epoch 60/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4654 - loss: 2.3742 - val_accuracy: 0.1194 - val_loss: 6.9312\n",
      "Epoch 61/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4617 - loss: 2.3934 - val_accuracy: 0.1159 - val_loss: 6.9809\n",
      "Epoch 62/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4543 - loss: 2.4194 - val_accuracy: 0.1202 - val_loss: 6.9496\n",
      "Epoch 63/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4718 - loss: 2.3431 - val_accuracy: 0.1144 - val_loss: 6.9824\n",
      "Epoch 64/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4434 - loss: 2.4814 - val_accuracy: 0.1174 - val_loss: 6.9693\n",
      "Epoch 65/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4171 - loss: 2.6844 - val_accuracy: 0.1191 - val_loss: 6.9415\n",
      "Epoch 66/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4406 - loss: 2.4830 - val_accuracy: 0.1193 - val_loss: 6.9464\n",
      "Epoch 67/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4464 - loss: 2.4688 - val_accuracy: 0.1142 - val_loss: 6.9912\n",
      "Epoch 68/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4484 - loss: 2.4421 - val_accuracy: 0.1161 - val_loss: 6.9633\n",
      "Epoch 69/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4638 - loss: 2.3713 - val_accuracy: 0.1186 - val_loss: 6.9893\n",
      "Epoch 70/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4386 - loss: 2.4991 - val_accuracy: 0.1187 - val_loss: 6.9794\n",
      "Epoch 71/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4770 - loss: 2.3037 - val_accuracy: 0.1154 - val_loss: 7.0217\n",
      "Epoch 72/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4676 - loss: 2.3542 - val_accuracy: 0.1159 - val_loss: 7.0413\n",
      "Epoch 73/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4662 - loss: 2.3481 - val_accuracy: 0.1141 - val_loss: 7.0466\n",
      "Epoch 74/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4764 - loss: 2.3024 - val_accuracy: 0.1180 - val_loss: 7.0588\n",
      "Epoch 75/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4715 - loss: 2.3210 - val_accuracy: 0.1116 - val_loss: 7.1242\n",
      "Epoch 76/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4755 - loss: 2.3000 - val_accuracy: 0.1170 - val_loss: 7.0952\n",
      "Epoch 77/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4800 - loss: 2.2789 - val_accuracy: 0.1181 - val_loss: 7.1024\n",
      "Epoch 78/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4764 - loss: 2.3036 - val_accuracy: 0.1168 - val_loss: 7.1220\n",
      "Epoch 79/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4808 - loss: 2.2684 - val_accuracy: 0.1162 - val_loss: 7.1284\n",
      "Epoch 80/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4827 - loss: 2.2627 - val_accuracy: 0.1030 - val_loss: 7.3224\n",
      "Epoch 81/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4454 - loss: 2.4525 - val_accuracy: 0.1134 - val_loss: 7.1429\n",
      "Epoch 82/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4851 - loss: 2.2535 - val_accuracy: 0.1168 - val_loss: 7.1515\n",
      "Epoch 83/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4436 - loss: 2.4625 - val_accuracy: 0.1119 - val_loss: 7.1491\n",
      "Epoch 84/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4514 - loss: 2.4187 - val_accuracy: 0.1134 - val_loss: 7.1341\n",
      "Epoch 85/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4580 - loss: 2.3899 - val_accuracy: 0.1127 - val_loss: 7.1329\n",
      "Epoch 86/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4374 - loss: 2.4964 - val_accuracy: 0.1178 - val_loss: 7.1100\n",
      "Epoch 87/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4863 - loss: 2.2415 - val_accuracy: 0.1171 - val_loss: 7.1537\n",
      "Epoch 88/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4830 - loss: 2.2593 - val_accuracy: 0.1157 - val_loss: 7.1559\n",
      "Epoch 89/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4912 - loss: 2.2113 - val_accuracy: 0.1151 - val_loss: 7.1934\n",
      "Epoch 90/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4721 - loss: 2.3063 - val_accuracy: 0.1135 - val_loss: 7.1880\n",
      "Epoch 91/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4894 - loss: 2.2212 - val_accuracy: 0.1130 - val_loss: 7.2099\n",
      "Epoch 92/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4912 - loss: 2.2112 - val_accuracy: 0.1139 - val_loss: 7.2178\n",
      "Epoch 93/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4791 - loss: 2.2647 - val_accuracy: 0.1115 - val_loss: 7.2365\n",
      "Epoch 94/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4904 - loss: 2.2071 - val_accuracy: 0.1150 - val_loss: 7.2216\n",
      "Epoch 95/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4978 - loss: 2.1743 - val_accuracy: 0.1124 - val_loss: 7.2651\n",
      "Epoch 96/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4906 - loss: 2.2066 - val_accuracy: 0.1145 - val_loss: 7.2565\n",
      "Epoch 97/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4680 - loss: 2.3173 - val_accuracy: 0.1156 - val_loss: 7.2380\n",
      "Epoch 98/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5001 - loss: 2.1646 - val_accuracy: 0.1135 - val_loss: 7.2809\n",
      "Epoch 99/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4812 - loss: 2.2497 - val_accuracy: 0.1141 - val_loss: 7.2805\n",
      "Epoch 100/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4754 - loss: 2.2962 - val_accuracy: 0.1116 - val_loss: 7.2709\n",
      "Epoch 101/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4969 - loss: 2.1749 - val_accuracy: 0.1136 - val_loss: 7.2915\n",
      "Epoch 102/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5049 - loss: 2.1435 - val_accuracy: 0.1103 - val_loss: 7.3039\n",
      "Epoch 103/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4989 - loss: 2.1713 - val_accuracy: 0.1141 - val_loss: 7.3282\n",
      "Epoch 104/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3596 - loss: 3.1818 - val_accuracy: 0.1073 - val_loss: 7.2488\n",
      "Epoch 105/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.3296 - loss: 3.1372 - val_accuracy: 0.1138 - val_loss: 7.1302\n",
      "Epoch 106/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4107 - loss: 2.6196 - val_accuracy: 0.1167 - val_loss: 7.1274\n",
      "Epoch 107/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4612 - loss: 2.3476 - val_accuracy: 0.1151 - val_loss: 7.1622\n",
      "Epoch 108/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4742 - loss: 2.2899 - val_accuracy: 0.1096 - val_loss: 7.2060\n",
      "Epoch 109/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4694 - loss: 2.3038 - val_accuracy: 0.1149 - val_loss: 7.2036\n",
      "Epoch 110/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4949 - loss: 2.1866 - val_accuracy: 0.1155 - val_loss: 7.2123\n",
      "Epoch 111/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4984 - loss: 2.1648 - val_accuracy: 0.1131 - val_loss: 7.2639\n",
      "Epoch 112/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4729 - loss: 2.3106 - val_accuracy: 0.1122 - val_loss: 7.2775\n",
      "Epoch 113/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4607 - loss: 2.3391 - val_accuracy: 0.1132 - val_loss: 7.2496\n",
      "Epoch 114/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5004 - loss: 2.1514 - val_accuracy: 0.1140 - val_loss: 7.2703\n",
      "Epoch 115/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4833 - loss: 2.2320 - val_accuracy: 0.1149 - val_loss: 7.2810\n",
      "Epoch 116/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4933 - loss: 2.1936 - val_accuracy: 0.1134 - val_loss: 7.2928\n",
      "Epoch 117/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4969 - loss: 2.1669 - val_accuracy: 0.1119 - val_loss: 7.2990\n",
      "Epoch 118/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4951 - loss: 2.1772 - val_accuracy: 0.1130 - val_loss: 7.3193\n",
      "Epoch 119/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4966 - loss: 2.1698 - val_accuracy: 0.1130 - val_loss: 7.3424\n",
      "Epoch 120/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4905 - loss: 2.2006 - val_accuracy: 0.1118 - val_loss: 7.3419\n",
      "Epoch 121/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5058 - loss: 2.1256 - val_accuracy: 0.1117 - val_loss: 7.3613\n",
      "Epoch 122/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4984 - loss: 2.1618 - val_accuracy: 0.1103 - val_loss: 7.3826\n",
      "Epoch 123/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4825 - loss: 2.2350 - val_accuracy: 0.1109 - val_loss: 7.3546\n",
      "Epoch 124/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4966 - loss: 2.1635 - val_accuracy: 0.1103 - val_loss: 7.3637\n",
      "Epoch 125/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4859 - loss: 2.2214 - val_accuracy: 0.1119 - val_loss: 7.3664\n",
      "Epoch 126/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5073 - loss: 2.1120 - val_accuracy: 0.1127 - val_loss: 7.3798\n",
      "Epoch 127/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5054 - loss: 2.1231 - val_accuracy: 0.1115 - val_loss: 7.3945\n",
      "Epoch 128/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4947 - loss: 2.1758 - val_accuracy: 0.1112 - val_loss: 7.3955\n",
      "Epoch 129/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5106 - loss: 2.0962 - val_accuracy: 0.1090 - val_loss: 7.3966\n",
      "Epoch 130/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4945 - loss: 2.1717 - val_accuracy: 0.1098 - val_loss: 7.4097\n",
      "Epoch 131/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5093 - loss: 2.1070 - val_accuracy: 0.1111 - val_loss: 7.4163\n",
      "Epoch 132/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5102 - loss: 2.0982 - val_accuracy: 0.1093 - val_loss: 7.4375\n",
      "Epoch 133/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5022 - loss: 2.1385 - val_accuracy: 0.1104 - val_loss: 7.4513\n",
      "Epoch 134/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5003 - loss: 2.1491 - val_accuracy: 0.1117 - val_loss: 7.4168\n",
      "Epoch 135/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4964 - loss: 2.1602 - val_accuracy: 0.1070 - val_loss: 7.4387\n",
      "Epoch 136/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5144 - loss: 2.0814 - val_accuracy: 0.1115 - val_loss: 7.4583\n",
      "Epoch 137/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4350 - loss: 2.5240 - val_accuracy: 0.1073 - val_loss: 7.4483\n",
      "Epoch 138/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4334 - loss: 2.4676 - val_accuracy: 0.1114 - val_loss: 7.3775\n",
      "Epoch 139/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4025 - loss: 2.6818 - val_accuracy: 0.1119 - val_loss: 7.3299\n",
      "Epoch 140/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4663 - loss: 2.3126 - val_accuracy: 0.1124 - val_loss: 7.3372\n",
      "Epoch 141/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4970 - loss: 2.1619 - val_accuracy: 0.1095 - val_loss: 7.3627\n",
      "Epoch 142/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4935 - loss: 2.1783 - val_accuracy: 0.1119 - val_loss: 7.3845\n",
      "Epoch 143/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4874 - loss: 2.1967 - val_accuracy: 0.1115 - val_loss: 7.3836\n",
      "Epoch 144/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5153 - loss: 2.0735 - val_accuracy: 0.1093 - val_loss: 7.4084\n",
      "Epoch 145/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5066 - loss: 2.1128 - val_accuracy: 0.1107 - val_loss: 7.4133\n",
      "Epoch 146/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5059 - loss: 2.1132 - val_accuracy: 0.1068 - val_loss: 7.4362\n",
      "Epoch 147/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4351 - loss: 2.5186 - val_accuracy: 0.1106 - val_loss: 7.3865\n",
      "Epoch 148/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.5089 - loss: 2.0999 - val_accuracy: 0.1117 - val_loss: 7.4015\n",
      "Epoch 149/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5042 - loss: 2.1208 - val_accuracy: 0.1111 - val_loss: 7.4179\n",
      "Epoch 150/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4755 - loss: 2.2645 - val_accuracy: 0.1098 - val_loss: 7.4109\n",
      "Epoch 151/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5079 - loss: 2.1030 - val_accuracy: 0.1112 - val_loss: 7.4413\n",
      "Epoch 152/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5110 - loss: 2.0869 - val_accuracy: 0.1081 - val_loss: 7.4412\n",
      "Epoch 153/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5188 - loss: 2.0490 - val_accuracy: 0.1094 - val_loss: 7.4659\n",
      "Epoch 154/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5183 - loss: 2.0490 - val_accuracy: 0.1088 - val_loss: 7.4874\n",
      "Epoch 155/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4906 - loss: 2.1818 - val_accuracy: 0.1080 - val_loss: 7.4646\n",
      "Epoch 156/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5078 - loss: 2.0940 - val_accuracy: 0.1085 - val_loss: 7.4761\n",
      "Epoch 157/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.5194 - loss: 2.0514 - val_accuracy: 0.1087 - val_loss: 7.4907\n",
      "Epoch 158/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5136 - loss: 2.0718 - val_accuracy: 0.1062 - val_loss: 7.5194\n",
      "Epoch 159/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5018 - loss: 2.1313 - val_accuracy: 0.1086 - val_loss: 7.5053\n",
      "Epoch 160/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5241 - loss: 2.0231 - val_accuracy: 0.1079 - val_loss: 7.5174\n",
      "Epoch 161/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4796 - loss: 2.2525 - val_accuracy: 0.1089 - val_loss: 7.4881\n",
      "Epoch 162/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5059 - loss: 2.1015 - val_accuracy: 0.1097 - val_loss: 7.5018\n",
      "Epoch 163/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5300 - loss: 1.9946 - val_accuracy: 0.1083 - val_loss: 7.5183\n",
      "Epoch 164/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.5045 - loss: 2.1147 - val_accuracy: 0.1097 - val_loss: 7.5145\n",
      "Epoch 165/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.4921 - loss: 2.1838 - val_accuracy: 0.1112 - val_loss: 7.4877\n",
      "Epoch 166/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5230 - loss: 2.0237 - val_accuracy: 0.1080 - val_loss: 7.5290\n",
      "Epoch 167/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5241 - loss: 2.0251 - val_accuracy: 0.1091 - val_loss: 7.5422\n",
      "Epoch 168/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.5142 - loss: 2.0699 - val_accuracy: 0.1068 - val_loss: 7.5416\n",
      "Epoch 169/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5198 - loss: 2.0361 - val_accuracy: 0.1081 - val_loss: 7.5588\n",
      "Epoch 170/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.5069 - loss: 2.1078 - val_accuracy: 0.1072 - val_loss: 7.5374\n",
      "\n",
      "--- Geração com SimpleRNN ---\n",
      "Semente: \"harry potter olhou para o castelo e\" | Temperatura: 0.7\n",
      "Texto gerado:\n",
      "------------------\n",
      "harry potter olhou para o castelo e uma vez mais um dia grande e os meus corredores era um traidor e passou chuva e o time da sonserina estava se encaminhando contra a parede de corrida para a torre o trem pôs um capítulo das duas bolas \n",
      "------------------\n",
      "\n",
      "✓ Modelo SimpleRNN salvo como 'modelo_simplernn_harry_potter.keras'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Modelo 1: SimpleRNN ---\n",
    "print(\"### Treinando o Modelo SimpleRNN ###\")\n",
    "rnn_model = create_model(SimpleRNN, vocab_size, seq_length)\n",
    "rnn_model.fit(X, y, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1)\n",
    "\n",
    "print(\"\\n--- Geração com SimpleRNN ---\")\n",
    "generate_text(rnn_model, tokenizer, seed)\n",
    "\n",
    "# Salvar o modelo\n",
    "rnn_model.save('modelo_simplernn_harry_potter.keras')\n",
    "print(\"✓ Modelo SimpleRNN salvo como 'modelo_simplernn_harry_potter.keras'\")\n",
    "\n",
    "del rnn_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7582534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Treinando o Modelo LSTM ###\n",
      "Epoch 1/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.0557 - loss: 6.7075 - val_accuracy: 0.0778 - val_loss: 6.3268\n",
      "Epoch 2/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.0891 - loss: 6.1053 - val_accuracy: 0.1004 - val_loss: 6.0536\n",
      "Epoch 3/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1094 - loss: 5.7684 - val_accuracy: 0.1134 - val_loss: 5.9106\n",
      "Epoch 4/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1218 - loss: 5.5324 - val_accuracy: 0.1232 - val_loss: 5.8362\n",
      "Epoch 5/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1323 - loss: 5.3365 - val_accuracy: 0.1291 - val_loss: 5.8133\n",
      "Epoch 6/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1426 - loss: 5.1600 - val_accuracy: 0.1329 - val_loss: 5.8133\n",
      "Epoch 7/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1504 - loss: 5.0071 - val_accuracy: 0.1383 - val_loss: 5.8337\n",
      "Epoch 8/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1570 - loss: 4.8761 - val_accuracy: 0.1399 - val_loss: 5.8677\n",
      "Epoch 9/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1628 - loss: 4.7614 - val_accuracy: 0.1477 - val_loss: 5.9177\n",
      "Epoch 10/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1687 - loss: 4.6611 - val_accuracy: 0.1485 - val_loss: 5.9720\n",
      "Epoch 11/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1742 - loss: 4.5677 - val_accuracy: 0.1491 - val_loss: 6.0260\n",
      "Epoch 12/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1796 - loss: 4.4812 - val_accuracy: 0.1534 - val_loss: 6.0743\n",
      "Epoch 13/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1851 - loss: 4.4016 - val_accuracy: 0.1518 - val_loss: 6.1345\n",
      "Epoch 14/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1921 - loss: 4.3264 - val_accuracy: 0.1523 - val_loss: 6.1845\n",
      "Epoch 15/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.1981 - loss: 4.2557 - val_accuracy: 0.1534 - val_loss: 6.2503\n",
      "Epoch 16/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2045 - loss: 4.1888 - val_accuracy: 0.1533 - val_loss: 6.3043\n",
      "Epoch 17/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2107 - loss: 4.1252 - val_accuracy: 0.1518 - val_loss: 6.3684\n",
      "Epoch 18/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2169 - loss: 4.0639 - val_accuracy: 0.1498 - val_loss: 6.4136\n",
      "Epoch 19/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2234 - loss: 4.0050 - val_accuracy: 0.1517 - val_loss: 6.4633\n",
      "Epoch 20/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2301 - loss: 3.9486 - val_accuracy: 0.1528 - val_loss: 6.5305\n",
      "Epoch 21/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2368 - loss: 3.8929 - val_accuracy: 0.1523 - val_loss: 6.5983\n",
      "Epoch 22/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2433 - loss: 3.8391 - val_accuracy: 0.1483 - val_loss: 6.6486\n",
      "Epoch 23/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2499 - loss: 3.7865 - val_accuracy: 0.1506 - val_loss: 6.6955\n",
      "Epoch 24/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2565 - loss: 3.7357 - val_accuracy: 0.1479 - val_loss: 6.7568\n",
      "Epoch 25/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2628 - loss: 3.6861 - val_accuracy: 0.1494 - val_loss: 6.8120\n",
      "Epoch 26/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2694 - loss: 3.6382 - val_accuracy: 0.1474 - val_loss: 6.8672\n",
      "Epoch 27/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2758 - loss: 3.5909 - val_accuracy: 0.1488 - val_loss: 6.9287\n",
      "Epoch 28/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2820 - loss: 3.5463 - val_accuracy: 0.1463 - val_loss: 6.9773\n",
      "Epoch 29/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2884 - loss: 3.5012 - val_accuracy: 0.1487 - val_loss: 7.0221\n",
      "Epoch 30/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2949 - loss: 3.4586 - val_accuracy: 0.1458 - val_loss: 7.0745\n",
      "Epoch 31/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3004 - loss: 3.4161 - val_accuracy: 0.1456 - val_loss: 7.1389\n",
      "Epoch 32/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3068 - loss: 3.3751 - val_accuracy: 0.1444 - val_loss: 7.1875\n",
      "Epoch 33/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3133 - loss: 3.3347 - val_accuracy: 0.1450 - val_loss: 7.2432\n",
      "Epoch 34/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3195 - loss: 3.2954 - val_accuracy: 0.1426 - val_loss: 7.3050\n",
      "Epoch 35/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3251 - loss: 3.2576 - val_accuracy: 0.1426 - val_loss: 7.3548\n",
      "Epoch 36/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3311 - loss: 3.2198 - val_accuracy: 0.1429 - val_loss: 7.4173\n",
      "Epoch 37/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3371 - loss: 3.1831 - val_accuracy: 0.1422 - val_loss: 7.4530\n",
      "Epoch 38/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3439 - loss: 3.1466 - val_accuracy: 0.1404 - val_loss: 7.5230\n",
      "Epoch 39/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3492 - loss: 3.1115 - val_accuracy: 0.1384 - val_loss: 7.5647\n",
      "Epoch 40/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3553 - loss: 3.0765 - val_accuracy: 0.1387 - val_loss: 7.6069\n",
      "Epoch 41/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3604 - loss: 3.0430 - val_accuracy: 0.1380 - val_loss: 7.6586\n",
      "Epoch 42/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3663 - loss: 3.0101 - val_accuracy: 0.1370 - val_loss: 7.7346\n",
      "Epoch 43/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3719 - loss: 2.9770 - val_accuracy: 0.1373 - val_loss: 7.7686\n",
      "Epoch 44/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3768 - loss: 2.9452 - val_accuracy: 0.1356 - val_loss: 7.8316\n",
      "Epoch 45/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3823 - loss: 2.9142 - val_accuracy: 0.1363 - val_loss: 7.8725\n",
      "Epoch 46/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3879 - loss: 2.8829 - val_accuracy: 0.1339 - val_loss: 7.9309\n",
      "Epoch 47/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3935 - loss: 2.8530 - val_accuracy: 0.1346 - val_loss: 7.9633\n",
      "Epoch 48/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.3985 - loss: 2.8242 - val_accuracy: 0.1333 - val_loss: 8.0427\n",
      "Epoch 49/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4026 - loss: 2.7941 - val_accuracy: 0.1327 - val_loss: 8.0866\n",
      "Epoch 50/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4084 - loss: 2.7669 - val_accuracy: 0.1346 - val_loss: 8.1380\n",
      "Epoch 51/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4144 - loss: 2.7375 - val_accuracy: 0.1340 - val_loss: 8.1935\n",
      "Epoch 52/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4174 - loss: 2.7116 - val_accuracy: 0.1311 - val_loss: 8.2568\n",
      "Epoch 53/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4232 - loss: 2.6854 - val_accuracy: 0.1308 - val_loss: 8.3016\n",
      "Epoch 54/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4285 - loss: 2.6584 - val_accuracy: 0.1307 - val_loss: 8.3394\n",
      "Epoch 55/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4327 - loss: 2.6322 - val_accuracy: 0.1315 - val_loss: 8.3955\n",
      "Epoch 56/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4372 - loss: 2.6066 - val_accuracy: 0.1272 - val_loss: 8.4420\n",
      "Epoch 57/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4423 - loss: 2.5823 - val_accuracy: 0.1274 - val_loss: 8.5161\n",
      "Epoch 58/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4465 - loss: 2.5600 - val_accuracy: 0.1268 - val_loss: 8.5578\n",
      "Epoch 59/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4507 - loss: 2.5336 - val_accuracy: 0.1274 - val_loss: 8.5823\n",
      "Epoch 60/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4554 - loss: 2.5110 - val_accuracy: 0.1255 - val_loss: 8.6580\n",
      "Epoch 61/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4595 - loss: 2.4869 - val_accuracy: 0.1269 - val_loss: 8.6905\n",
      "Epoch 62/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4641 - loss: 2.4654 - val_accuracy: 0.1247 - val_loss: 8.7731\n",
      "Epoch 63/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4678 - loss: 2.4435 - val_accuracy: 0.1236 - val_loss: 8.7981\n",
      "Epoch 64/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4720 - loss: 2.4216 - val_accuracy: 0.1248 - val_loss: 8.8398\n",
      "Epoch 65/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4766 - loss: 2.4007 - val_accuracy: 0.1235 - val_loss: 8.8899\n",
      "Epoch 66/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4800 - loss: 2.3802 - val_accuracy: 0.1225 - val_loss: 8.9488\n",
      "Epoch 67/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4832 - loss: 2.3612 - val_accuracy: 0.1232 - val_loss: 8.9864\n",
      "Epoch 68/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4874 - loss: 2.3387 - val_accuracy: 0.1224 - val_loss: 9.0643\n",
      "Epoch 69/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4914 - loss: 2.3201 - val_accuracy: 0.1215 - val_loss: 9.1036\n",
      "Epoch 70/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4960 - loss: 2.3005 - val_accuracy: 0.1220 - val_loss: 9.1325\n",
      "Epoch 71/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.4979 - loss: 2.2823 - val_accuracy: 0.1228 - val_loss: 9.1989\n",
      "Epoch 72/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5025 - loss: 2.2616 - val_accuracy: 0.1231 - val_loss: 9.2391\n",
      "Epoch 73/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5061 - loss: 2.2446 - val_accuracy: 0.1188 - val_loss: 9.2701\n",
      "Epoch 74/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5101 - loss: 2.2278 - val_accuracy: 0.1201 - val_loss: 9.3234\n",
      "Epoch 75/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5136 - loss: 2.2090 - val_accuracy: 0.1204 - val_loss: 9.3726\n",
      "Epoch 76/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5157 - loss: 2.1930 - val_accuracy: 0.1208 - val_loss: 9.4124\n",
      "Epoch 77/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5200 - loss: 2.1760 - val_accuracy: 0.1210 - val_loss: 9.4559\n",
      "Epoch 78/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5228 - loss: 2.1581 - val_accuracy: 0.1195 - val_loss: 9.5093\n",
      "Epoch 79/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5253 - loss: 2.1420 - val_accuracy: 0.1168 - val_loss: 9.5411\n",
      "Epoch 80/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5290 - loss: 2.1268 - val_accuracy: 0.1165 - val_loss: 9.5886\n",
      "Epoch 81/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5319 - loss: 2.1116 - val_accuracy: 0.1186 - val_loss: 9.6672\n",
      "Epoch 82/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5354 - loss: 2.0966 - val_accuracy: 0.1190 - val_loss: 9.7029\n",
      "Epoch 83/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5378 - loss: 2.0811 - val_accuracy: 0.1169 - val_loss: 9.7355\n",
      "Epoch 84/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5414 - loss: 2.0672 - val_accuracy: 0.1155 - val_loss: 9.7622\n",
      "Epoch 85/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5432 - loss: 2.0532 - val_accuracy: 0.1147 - val_loss: 9.8295\n",
      "Epoch 86/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5469 - loss: 2.0381 - val_accuracy: 0.1153 - val_loss: 9.8684\n",
      "Epoch 87/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5497 - loss: 2.0236 - val_accuracy: 0.1173 - val_loss: 9.8975\n",
      "Epoch 88/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5519 - loss: 2.0099 - val_accuracy: 0.1166 - val_loss: 9.9506\n",
      "Epoch 89/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5556 - loss: 1.9971 - val_accuracy: 0.1158 - val_loss: 10.0116\n",
      "Epoch 90/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5577 - loss: 1.9851 - val_accuracy: 0.1136 - val_loss: 10.0483\n",
      "Epoch 91/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5607 - loss: 1.9689 - val_accuracy: 0.1132 - val_loss: 10.0913\n",
      "Epoch 92/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5623 - loss: 1.9587 - val_accuracy: 0.1144 - val_loss: 10.1155\n",
      "Epoch 93/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5652 - loss: 1.9460 - val_accuracy: 0.1136 - val_loss: 10.1475\n",
      "Epoch 94/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5683 - loss: 1.9319 - val_accuracy: 0.1142 - val_loss: 10.2036\n",
      "Epoch 95/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5695 - loss: 1.9210 - val_accuracy: 0.1134 - val_loss: 10.2393\n",
      "Epoch 96/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5723 - loss: 1.9091 - val_accuracy: 0.1124 - val_loss: 10.2905\n",
      "Epoch 97/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5755 - loss: 1.8961 - val_accuracy: 0.1122 - val_loss: 10.3398\n",
      "Epoch 98/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5761 - loss: 1.8869 - val_accuracy: 0.1117 - val_loss: 10.3759\n",
      "Epoch 99/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5790 - loss: 1.8751 - val_accuracy: 0.1118 - val_loss: 10.3869\n",
      "Epoch 100/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5813 - loss: 1.8646 - val_accuracy: 0.1116 - val_loss: 10.4694\n",
      "Epoch 101/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5831 - loss: 1.8535 - val_accuracy: 0.1127 - val_loss: 10.4863\n",
      "Epoch 102/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5861 - loss: 1.8409 - val_accuracy: 0.1129 - val_loss: 10.5291\n",
      "Epoch 103/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5878 - loss: 1.8314 - val_accuracy: 0.1121 - val_loss: 10.5505\n",
      "Epoch 104/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5902 - loss: 1.8217 - val_accuracy: 0.1114 - val_loss: 10.6037\n",
      "Epoch 105/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5923 - loss: 1.8117 - val_accuracy: 0.1130 - val_loss: 10.6435\n",
      "Epoch 106/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5948 - loss: 1.8018 - val_accuracy: 0.1094 - val_loss: 10.6721\n",
      "Epoch 107/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5959 - loss: 1.7923 - val_accuracy: 0.1112 - val_loss: 10.7426\n",
      "Epoch 108/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5981 - loss: 1.7821 - val_accuracy: 0.1098 - val_loss: 10.7373\n",
      "Epoch 109/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.5993 - loss: 1.7745 - val_accuracy: 0.1115 - val_loss: 10.7883\n",
      "Epoch 110/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6019 - loss: 1.7634 - val_accuracy: 0.1077 - val_loss: 10.8230\n",
      "Epoch 111/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6027 - loss: 1.7561 - val_accuracy: 0.1084 - val_loss: 10.8716\n",
      "Epoch 112/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6048 - loss: 1.7488 - val_accuracy: 0.1099 - val_loss: 10.9024\n",
      "Epoch 113/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6067 - loss: 1.7389 - val_accuracy: 0.1081 - val_loss: 10.9502\n",
      "Epoch 114/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6090 - loss: 1.7292 - val_accuracy: 0.1108 - val_loss: 10.9812\n",
      "Epoch 115/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6111 - loss: 1.7201 - val_accuracy: 0.1087 - val_loss: 11.0089\n",
      "Epoch 116/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6123 - loss: 1.7124 - val_accuracy: 0.1067 - val_loss: 11.0268\n",
      "Epoch 117/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6137 - loss: 1.7049 - val_accuracy: 0.1091 - val_loss: 11.0759\n",
      "Epoch 118/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6152 - loss: 1.6963 - val_accuracy: 0.1075 - val_loss: 11.1189\n",
      "Epoch 119/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6162 - loss: 1.6880 - val_accuracy: 0.1077 - val_loss: 11.1576\n",
      "Epoch 120/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6177 - loss: 1.6811 - val_accuracy: 0.1074 - val_loss: 11.1804\n",
      "Epoch 121/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6200 - loss: 1.6725 - val_accuracy: 0.1067 - val_loss: 11.2243\n",
      "Epoch 122/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6208 - loss: 1.6687 - val_accuracy: 0.1068 - val_loss: 11.2823\n",
      "Epoch 123/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6225 - loss: 1.6588 - val_accuracy: 0.1075 - val_loss: 11.2751\n",
      "Epoch 124/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6251 - loss: 1.6489 - val_accuracy: 0.1076 - val_loss: 11.3371\n",
      "Epoch 125/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6258 - loss: 1.6444 - val_accuracy: 0.1066 - val_loss: 11.3706\n",
      "Epoch 126/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6270 - loss: 1.6383 - val_accuracy: 0.1059 - val_loss: 11.3990\n",
      "Epoch 127/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6287 - loss: 1.6288 - val_accuracy: 0.1070 - val_loss: 11.4055\n",
      "Epoch 128/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6289 - loss: 1.6247 - val_accuracy: 0.1086 - val_loss: 11.4416\n",
      "Epoch 129/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6308 - loss: 1.6172 - val_accuracy: 0.1051 - val_loss: 11.4594\n",
      "Epoch 130/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6330 - loss: 1.6080 - val_accuracy: 0.1066 - val_loss: 11.5194\n",
      "Epoch 131/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6340 - loss: 1.6020 - val_accuracy: 0.1074 - val_loss: 11.5512\n",
      "Epoch 132/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6354 - loss: 1.5976 - val_accuracy: 0.1058 - val_loss: 11.6047\n",
      "Epoch 133/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6369 - loss: 1.5917 - val_accuracy: 0.1057 - val_loss: 11.6044\n",
      "Epoch 134/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6372 - loss: 1.5859 - val_accuracy: 0.1051 - val_loss: 11.6581\n",
      "Epoch 135/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6371 - loss: 1.5866 - val_accuracy: 0.1051 - val_loss: 11.6655\n",
      "Epoch 136/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6399 - loss: 1.5724 - val_accuracy: 0.1041 - val_loss: 11.6868\n",
      "Epoch 137/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6421 - loss: 1.5660 - val_accuracy: 0.1035 - val_loss: 11.7328\n",
      "Epoch 138/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6420 - loss: 1.5618 - val_accuracy: 0.1042 - val_loss: 11.7609\n",
      "Epoch 139/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6440 - loss: 1.5560 - val_accuracy: 0.1028 - val_loss: 11.8034\n",
      "Epoch 140/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6443 - loss: 1.5507 - val_accuracy: 0.1034 - val_loss: 11.8495\n",
      "Epoch 141/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6457 - loss: 1.5431 - val_accuracy: 0.1034 - val_loss: 11.8791\n",
      "Epoch 142/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6465 - loss: 1.5395 - val_accuracy: 0.1035 - val_loss: 11.9243\n",
      "Epoch 143/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6483 - loss: 1.5344 - val_accuracy: 0.1014 - val_loss: 11.9465\n",
      "Epoch 144/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6491 - loss: 1.5274 - val_accuracy: 0.1032 - val_loss: 11.9792\n",
      "Epoch 145/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6502 - loss: 1.5229 - val_accuracy: 0.1032 - val_loss: 11.9824\n",
      "Epoch 146/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6506 - loss: 1.5182 - val_accuracy: 0.1032 - val_loss: 12.0228\n",
      "Epoch 147/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6525 - loss: 1.5112 - val_accuracy: 0.1040 - val_loss: 12.0747\n",
      "Epoch 148/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6524 - loss: 1.5084 - val_accuracy: 0.1029 - val_loss: 12.0662\n",
      "Epoch 149/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6545 - loss: 1.5000 - val_accuracy: 0.1027 - val_loss: 12.1048\n",
      "Epoch 150/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6553 - loss: 1.4983 - val_accuracy: 0.1035 - val_loss: 12.1384\n",
      "Epoch 151/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6573 - loss: 1.4908 - val_accuracy: 0.1028 - val_loss: 12.1766\n",
      "Epoch 152/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6566 - loss: 1.4916 - val_accuracy: 0.1030 - val_loss: 12.2028\n",
      "Epoch 153/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6578 - loss: 1.4816 - val_accuracy: 0.1025 - val_loss: 12.2225\n",
      "Epoch 154/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6581 - loss: 1.4793 - val_accuracy: 0.1016 - val_loss: 12.2426\n",
      "Epoch 155/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6585 - loss: 1.4759 - val_accuracy: 0.1017 - val_loss: 12.2967\n",
      "Epoch 156/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6608 - loss: 1.4718 - val_accuracy: 0.1019 - val_loss: 12.3164\n",
      "Epoch 157/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6607 - loss: 1.4681 - val_accuracy: 0.1014 - val_loss: 12.3349\n",
      "Epoch 158/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6625 - loss: 1.4591 - val_accuracy: 0.1027 - val_loss: 12.3411\n",
      "Epoch 159/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6629 - loss: 1.4571 - val_accuracy: 0.1012 - val_loss: 12.3731\n",
      "Epoch 160/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6638 - loss: 1.4552 - val_accuracy: 0.0990 - val_loss: 12.4246\n",
      "Epoch 161/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6639 - loss: 1.4528 - val_accuracy: 0.1026 - val_loss: 12.4688\n",
      "Epoch 162/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6646 - loss: 1.4458 - val_accuracy: 0.1025 - val_loss: 12.4631\n",
      "Epoch 163/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6666 - loss: 1.4416 - val_accuracy: 0.1019 - val_loss: 12.4876\n",
      "Epoch 164/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6657 - loss: 1.4412 - val_accuracy: 0.1018 - val_loss: 12.5270\n",
      "Epoch 165/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6676 - loss: 1.4323 - val_accuracy: 0.1004 - val_loss: 12.5254\n",
      "Epoch 166/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6678 - loss: 1.4304 - val_accuracy: 0.1005 - val_loss: 12.5654\n",
      "Epoch 167/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6682 - loss: 1.4280 - val_accuracy: 0.1008 - val_loss: 12.5910\n",
      "Epoch 168/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6696 - loss: 1.4224 - val_accuracy: 0.1005 - val_loss: 12.6367\n",
      "Epoch 169/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6710 - loss: 1.4171 - val_accuracy: 0.1017 - val_loss: 12.6546\n",
      "Epoch 170/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.6706 - loss: 1.4153 - val_accuracy: 0.1012 - val_loss: 12.6665\n",
      "\n",
      "--- Geração com LSTM ---\n",
      "Semente: \"harry potter olhou para o castelo e\" | Temperatura: 0.7\n",
      "Texto gerado:\n",
      "------------------\n",
      "harry potter olhou para o castelo e saiu palmas e rony que estava sentados alguém deitado na ala hospitalar devia ter visto o que estava acontecendo sentia o rosto de harry mas pensou que o olhar snape não podia deixar de sentir a vassoura de hermione porque \n",
      "------------------\n",
      "\n",
      "Modelo LSTM salvo como 'modelo_lstm_harry_potter.keras'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Modelo 2: LSTM ---\n",
    "print(\"\\n### Treinando o Modelo LSTM ###\")\n",
    "lstm_model = create_model(LSTM, vocab_size, seq_length)\n",
    "lstm_model.fit(X, y, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1)\n",
    "\n",
    "print(\"\\n--- Geração com LSTM ---\")\n",
    "generate_text(lstm_model, tokenizer, seed)\n",
    "\n",
    "# Salvar o modelo\n",
    "lstm_model.save('modelo_lstm_harry_potter.keras')\n",
    "print(\"Modelo LSTM salvo como 'modelo_lstm_harry_potter.keras'\")\n",
    "\n",
    "del lstm_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d275a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Treinando o Modelo GRU ###\n",
      "Epoch 1/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.0502 - loss: 6.9398 - val_accuracy: 0.0828 - val_loss: 6.3740\n",
      "Epoch 2/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1035 - loss: 5.9616 - val_accuracy: 0.1273 - val_loss: 5.7489\n",
      "Epoch 3/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1376 - loss: 5.4135 - val_accuracy: 0.1434 - val_loss: 5.6021\n",
      "Epoch 4/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1567 - loss: 5.0530 - val_accuracy: 0.1487 - val_loss: 5.5634\n",
      "Epoch 5/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1730 - loss: 4.7612 - val_accuracy: 0.1508 - val_loss: 5.5849\n",
      "Epoch 6/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.1879 - loss: 4.5089 - val_accuracy: 0.1533 - val_loss: 5.6199\n",
      "Epoch 7/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2049 - loss: 4.2859 - val_accuracy: 0.1497 - val_loss: 5.6769\n",
      "Epoch 8/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2228 - loss: 4.0831 - val_accuracy: 0.1540 - val_loss: 5.7352\n",
      "Epoch 9/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2443 - loss: 3.8949 - val_accuracy: 0.1508 - val_loss: 5.8085\n",
      "Epoch 10/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2662 - loss: 3.7230 - val_accuracy: 0.1459 - val_loss: 5.8764\n",
      "Epoch 11/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.2894 - loss: 3.5627 - val_accuracy: 0.1461 - val_loss: 5.9369\n",
      "Epoch 12/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3115 - loss: 3.4162 - val_accuracy: 0.1412 - val_loss: 6.0171\n",
      "Epoch 13/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3328 - loss: 3.2793 - val_accuracy: 0.1389 - val_loss: 6.0882\n",
      "Epoch 14/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3527 - loss: 3.1549 - val_accuracy: 0.1389 - val_loss: 6.1683\n",
      "Epoch 15/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3712 - loss: 3.0371 - val_accuracy: 0.1368 - val_loss: 6.2557\n",
      "Epoch 16/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.3900 - loss: 2.9283 - val_accuracy: 0.1369 - val_loss: 6.3312\n",
      "Epoch 17/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4070 - loss: 2.8270 - val_accuracy: 0.1332 - val_loss: 6.4302\n",
      "Epoch 18/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4229 - loss: 2.7304 - val_accuracy: 0.1305 - val_loss: 6.5090\n",
      "Epoch 19/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4386 - loss: 2.6409 - val_accuracy: 0.1287 - val_loss: 6.6062\n",
      "Epoch 20/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4536 - loss: 2.5553 - val_accuracy: 0.1278 - val_loss: 6.6874\n",
      "Epoch 21/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4683 - loss: 2.4757 - val_accuracy: 0.1229 - val_loss: 6.7824\n",
      "Epoch 22/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4814 - loss: 2.4027 - val_accuracy: 0.1213 - val_loss: 6.8585\n",
      "Epoch 23/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4945 - loss: 2.3310 - val_accuracy: 0.1222 - val_loss: 6.9482\n",
      "Epoch 24/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5068 - loss: 2.2649 - val_accuracy: 0.1207 - val_loss: 7.0398\n",
      "Epoch 25/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5176 - loss: 2.2020 - val_accuracy: 0.1176 - val_loss: 7.1375\n",
      "Epoch 26/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5299 - loss: 2.1434 - val_accuracy: 0.1201 - val_loss: 7.2167\n",
      "Epoch 27/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5404 - loss: 2.0872 - val_accuracy: 0.1190 - val_loss: 7.2988\n",
      "Epoch 28/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5505 - loss: 2.0340 - val_accuracy: 0.1151 - val_loss: 7.3986\n",
      "Epoch 29/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5599 - loss: 1.9853 - val_accuracy: 0.1149 - val_loss: 7.4751\n",
      "Epoch 30/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5693 - loss: 1.9369 - val_accuracy: 0.1129 - val_loss: 7.5582\n",
      "Epoch 31/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5769 - loss: 1.8953 - val_accuracy: 0.1132 - val_loss: 7.6352\n",
      "Epoch 32/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5858 - loss: 1.8515 - val_accuracy: 0.1110 - val_loss: 7.7269\n",
      "Epoch 33/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.5926 - loss: 1.8129 - val_accuracy: 0.1085 - val_loss: 7.8069\n",
      "Epoch 34/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6008 - loss: 1.7731 - val_accuracy: 0.1083 - val_loss: 7.8848\n",
      "Epoch 35/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6073 - loss: 1.7378 - val_accuracy: 0.1087 - val_loss: 7.9716\n",
      "Epoch 36/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6140 - loss: 1.7035 - val_accuracy: 0.1091 - val_loss: 8.0282\n",
      "Epoch 37/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6199 - loss: 1.6742 - val_accuracy: 0.1068 - val_loss: 8.1171\n",
      "Epoch 38/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6256 - loss: 1.6413 - val_accuracy: 0.1030 - val_loss: 8.2000\n",
      "Epoch 39/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6314 - loss: 1.6116 - val_accuracy: 0.1065 - val_loss: 8.2713\n",
      "Epoch 40/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6356 - loss: 1.5894 - val_accuracy: 0.1032 - val_loss: 8.3238\n",
      "Epoch 41/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6426 - loss: 1.5560 - val_accuracy: 0.1053 - val_loss: 8.3916\n",
      "Epoch 42/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6468 - loss: 1.5321 - val_accuracy: 0.1028 - val_loss: 8.4915\n",
      "Epoch 43/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6531 - loss: 1.5081 - val_accuracy: 0.1037 - val_loss: 8.5496\n",
      "Epoch 44/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6557 - loss: 1.4865 - val_accuracy: 0.1032 - val_loss: 8.6292\n",
      "Epoch 45/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6603 - loss: 1.4625 - val_accuracy: 0.1006 - val_loss: 8.6884\n",
      "Epoch 46/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6643 - loss: 1.4433 - val_accuracy: 0.1009 - val_loss: 8.7386\n",
      "Epoch 47/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6686 - loss: 1.4238 - val_accuracy: 0.0993 - val_loss: 8.8307\n",
      "Epoch 48/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6721 - loss: 1.4021 - val_accuracy: 0.1000 - val_loss: 8.8928\n",
      "Epoch 49/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6751 - loss: 1.3863 - val_accuracy: 0.0981 - val_loss: 8.9451\n",
      "Epoch 50/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6797 - loss: 1.3644 - val_accuracy: 0.0986 - val_loss: 9.0111\n",
      "Epoch 51/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6818 - loss: 1.3500 - val_accuracy: 0.0991 - val_loss: 9.0535\n",
      "Epoch 52/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6850 - loss: 1.3340 - val_accuracy: 0.0988 - val_loss: 9.1246\n",
      "Epoch 53/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6889 - loss: 1.3184 - val_accuracy: 0.0975 - val_loss: 9.1882\n",
      "Epoch 54/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6906 - loss: 1.3057 - val_accuracy: 0.0987 - val_loss: 9.2380\n",
      "Epoch 55/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6944 - loss: 1.2909 - val_accuracy: 0.0977 - val_loss: 9.2981\n",
      "Epoch 56/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6961 - loss: 1.2771 - val_accuracy: 0.0980 - val_loss: 9.3307\n",
      "Epoch 57/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.6981 - loss: 1.2667 - val_accuracy: 0.0990 - val_loss: 9.3881\n",
      "Epoch 58/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7013 - loss: 1.2498 - val_accuracy: 0.0976 - val_loss: 9.4332\n",
      "Epoch 59/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7025 - loss: 1.2387 - val_accuracy: 0.0976 - val_loss: 9.4991\n",
      "Epoch 60/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7052 - loss: 1.2279 - val_accuracy: 0.0970 - val_loss: 9.5468\n",
      "Epoch 61/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7083 - loss: 1.2131 - val_accuracy: 0.0965 - val_loss: 9.6107\n",
      "Epoch 62/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7103 - loss: 1.2033 - val_accuracy: 0.0964 - val_loss: 9.6276\n",
      "Epoch 63/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7113 - loss: 1.1976 - val_accuracy: 0.0966 - val_loss: 9.6891\n",
      "Epoch 64/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7141 - loss: 1.1845 - val_accuracy: 0.0966 - val_loss: 9.7381\n",
      "Epoch 65/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7156 - loss: 1.1749 - val_accuracy: 0.0982 - val_loss: 9.7887\n",
      "Epoch 66/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7169 - loss: 1.1664 - val_accuracy: 0.0962 - val_loss: 9.8335\n",
      "Epoch 67/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7197 - loss: 1.1528 - val_accuracy: 0.0977 - val_loss: 9.8761\n",
      "Epoch 68/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7184 - loss: 1.1562 - val_accuracy: 0.0972 - val_loss: 9.9202\n",
      "Epoch 69/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7220 - loss: 1.1386 - val_accuracy: 0.0979 - val_loss: 9.9824\n",
      "Epoch 70/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7234 - loss: 1.1311 - val_accuracy: 0.0953 - val_loss: 10.0160\n",
      "Epoch 71/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7253 - loss: 1.1237 - val_accuracy: 0.0945 - val_loss: 10.0423\n",
      "Epoch 72/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7273 - loss: 1.1156 - val_accuracy: 0.0944 - val_loss: 10.0920\n",
      "Epoch 73/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7295 - loss: 1.1075 - val_accuracy: 0.0926 - val_loss: 10.1586\n",
      "Epoch 74/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7311 - loss: 1.0974 - val_accuracy: 0.0927 - val_loss: 10.2068\n",
      "Epoch 75/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7299 - loss: 1.0977 - val_accuracy: 0.0950 - val_loss: 10.2368\n",
      "Epoch 76/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7329 - loss: 1.0856 - val_accuracy: 0.0945 - val_loss: 10.2557\n",
      "Epoch 77/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7326 - loss: 1.0857 - val_accuracy: 0.0941 - val_loss: 10.2985\n",
      "Epoch 78/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7348 - loss: 1.0729 - val_accuracy: 0.0935 - val_loss: 10.3380\n",
      "Epoch 79/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7350 - loss: 1.0716 - val_accuracy: 0.0930 - val_loss: 10.3779\n",
      "Epoch 80/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7385 - loss: 1.0585 - val_accuracy: 0.0946 - val_loss: 10.4126\n",
      "Epoch 81/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7390 - loss: 1.0546 - val_accuracy: 0.0947 - val_loss: 10.4517\n",
      "Epoch 82/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7388 - loss: 1.0508 - val_accuracy: 0.0929 - val_loss: 10.5075\n",
      "Epoch 83/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7398 - loss: 1.0456 - val_accuracy: 0.0918 - val_loss: 10.5277\n",
      "Epoch 84/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7406 - loss: 1.0416 - val_accuracy: 0.0952 - val_loss: 10.5569\n",
      "Epoch 85/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7432 - loss: 1.0288 - val_accuracy: 0.0932 - val_loss: 10.5985\n",
      "Epoch 86/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 1.0315 - val_accuracy: 0.0925 - val_loss: 10.6337\n",
      "Epoch 87/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7443 - loss: 1.0216 - val_accuracy: 0.0931 - val_loss: 10.6808\n",
      "Epoch 88/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7446 - loss: 1.0187 - val_accuracy: 0.0930 - val_loss: 10.6824\n",
      "Epoch 89/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7459 - loss: 1.0114 - val_accuracy: 0.0929 - val_loss: 10.7244\n",
      "Epoch 90/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7465 - loss: 1.0080 - val_accuracy: 0.0920 - val_loss: 10.7681\n",
      "Epoch 91/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7478 - loss: 1.0056 - val_accuracy: 0.0921 - val_loss: 10.7985\n",
      "Epoch 92/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7492 - loss: 0.9957 - val_accuracy: 0.0918 - val_loss: 10.8268\n",
      "Epoch 93/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7492 - loss: 0.9938 - val_accuracy: 0.0916 - val_loss: 10.8556\n",
      "Epoch 94/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7490 - loss: 0.9920 - val_accuracy: 0.0940 - val_loss: 10.8853\n",
      "Epoch 95/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7516 - loss: 0.9850 - val_accuracy: 0.0930 - val_loss: 10.9270\n",
      "Epoch 96/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7505 - loss: 0.9859 - val_accuracy: 0.0921 - val_loss: 10.9480\n",
      "Epoch 97/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7544 - loss: 0.9731 - val_accuracy: 0.0905 - val_loss: 10.9787\n",
      "Epoch 98/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7530 - loss: 0.9727 - val_accuracy: 0.0918 - val_loss: 10.9997\n",
      "Epoch 99/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7533 - loss: 0.9733 - val_accuracy: 0.0919 - val_loss: 11.0483\n",
      "Epoch 100/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7555 - loss: 0.9656 - val_accuracy: 0.0922 - val_loss: 11.0549\n",
      "Epoch 101/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7563 - loss: 0.9619 - val_accuracy: 0.0917 - val_loss: 11.0795\n",
      "Epoch 102/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7556 - loss: 0.9602 - val_accuracy: 0.0933 - val_loss: 11.1119\n",
      "Epoch 103/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7566 - loss: 0.9549 - val_accuracy: 0.0924 - val_loss: 11.1411\n",
      "Epoch 104/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7587 - loss: 0.9501 - val_accuracy: 0.0908 - val_loss: 11.1611\n",
      "Epoch 105/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7567 - loss: 0.9528 - val_accuracy: 0.0926 - val_loss: 11.1614\n",
      "Epoch 106/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7606 - loss: 0.9379 - val_accuracy: 0.0915 - val_loss: 11.2041\n",
      "Epoch 107/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7593 - loss: 0.9385 - val_accuracy: 0.0920 - val_loss: 11.2448\n",
      "Epoch 108/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7598 - loss: 0.9392 - val_accuracy: 0.0923 - val_loss: 11.2528\n",
      "Epoch 109/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7625 - loss: 0.9309 - val_accuracy: 0.0907 - val_loss: 11.2750\n",
      "Epoch 110/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7619 - loss: 0.9296 - val_accuracy: 0.0907 - val_loss: 11.3161\n",
      "Epoch 111/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7616 - loss: 0.9290 - val_accuracy: 0.0897 - val_loss: 11.3380\n",
      "Epoch 112/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7622 - loss: 0.9263 - val_accuracy: 0.0904 - val_loss: 11.3666\n",
      "Epoch 113/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7633 - loss: 0.9213 - val_accuracy: 0.0887 - val_loss: 11.3795\n",
      "Epoch 114/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7653 - loss: 0.9131 - val_accuracy: 0.0898 - val_loss: 11.4074\n",
      "Epoch 115/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7628 - loss: 0.9226 - val_accuracy: 0.0903 - val_loss: 11.4068\n",
      "Epoch 116/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7645 - loss: 0.9151 - val_accuracy: 0.0906 - val_loss: 11.4399\n",
      "Epoch 117/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7672 - loss: 0.9029 - val_accuracy: 0.0908 - val_loss: 11.4904\n",
      "Epoch 118/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7672 - loss: 0.9056 - val_accuracy: 0.0905 - val_loss: 11.5047\n",
      "Epoch 119/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7659 - loss: 0.9049 - val_accuracy: 0.0887 - val_loss: 11.5143\n",
      "Epoch 120/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7670 - loss: 0.9022 - val_accuracy: 0.0903 - val_loss: 11.5143\n",
      "Epoch 121/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7685 - loss: 0.8955 - val_accuracy: 0.0905 - val_loss: 11.5376\n",
      "Epoch 122/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7697 - loss: 0.8920 - val_accuracy: 0.0928 - val_loss: 11.5976\n",
      "Epoch 123/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7706 - loss: 0.8859 - val_accuracy: 0.0900 - val_loss: 11.5791\n",
      "Epoch 124/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7713 - loss: 0.8838 - val_accuracy: 0.0902 - val_loss: 11.6025\n",
      "Epoch 125/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7657 - loss: 0.9024 - val_accuracy: 0.0886 - val_loss: 11.6210\n",
      "Epoch 126/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7715 - loss: 0.8826 - val_accuracy: 0.0902 - val_loss: 11.6529\n",
      "Epoch 127/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7708 - loss: 0.8833 - val_accuracy: 0.0915 - val_loss: 11.6566\n",
      "Epoch 128/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7716 - loss: 0.8786 - val_accuracy: 0.0907 - val_loss: 11.6860\n",
      "Epoch 129/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7700 - loss: 0.8821 - val_accuracy: 0.0903 - val_loss: 11.7307\n",
      "Epoch 130/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7716 - loss: 0.8768 - val_accuracy: 0.0901 - val_loss: 11.7400\n",
      "Epoch 131/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7699 - loss: 0.8784 - val_accuracy: 0.0905 - val_loss: 11.7643\n",
      "Epoch 132/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7749 - loss: 0.8672 - val_accuracy: 0.0885 - val_loss: 11.7908\n",
      "Epoch 133/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7716 - loss: 0.8735 - val_accuracy: 0.0884 - val_loss: 11.7963\n",
      "Epoch 134/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7722 - loss: 0.8717 - val_accuracy: 0.0884 - val_loss: 11.8391\n",
      "Epoch 135/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7758 - loss: 0.8607 - val_accuracy: 0.0908 - val_loss: 11.8401\n",
      "Epoch 136/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7745 - loss: 0.8620 - val_accuracy: 0.0912 - val_loss: 11.8520\n",
      "Epoch 137/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7758 - loss: 0.8581 - val_accuracy: 0.0881 - val_loss: 11.8987\n",
      "Epoch 138/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7767 - loss: 0.8549 - val_accuracy: 0.0915 - val_loss: 11.8844\n",
      "Epoch 139/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7742 - loss: 0.8628 - val_accuracy: 0.0881 - val_loss: 11.9121\n",
      "Epoch 140/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7745 - loss: 0.8599 - val_accuracy: 0.0890 - val_loss: 11.9294\n",
      "Epoch 141/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7784 - loss: 0.8476 - val_accuracy: 0.0886 - val_loss: 11.9517\n",
      "Epoch 142/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7749 - loss: 0.8535 - val_accuracy: 0.0884 - val_loss: 11.9795\n",
      "Epoch 143/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7767 - loss: 0.8498 - val_accuracy: 0.0883 - val_loss: 11.9969\n",
      "Epoch 144/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7788 - loss: 0.8407 - val_accuracy: 0.0885 - val_loss: 12.0085\n",
      "Epoch 145/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7753 - loss: 0.8552 - val_accuracy: 0.0911 - val_loss: 12.0432\n",
      "Epoch 146/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7789 - loss: 0.8404 - val_accuracy: 0.0907 - val_loss: 12.0575\n",
      "Epoch 147/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7796 - loss: 0.8366 - val_accuracy: 0.0888 - val_loss: 12.0421\n",
      "Epoch 148/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7794 - loss: 0.8395 - val_accuracy: 0.0883 - val_loss: 12.0819\n",
      "Epoch 149/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7795 - loss: 0.8359 - val_accuracy: 0.0896 - val_loss: 12.0855\n",
      "Epoch 150/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7781 - loss: 0.8397 - val_accuracy: 0.0870 - val_loss: 12.1056\n",
      "Epoch 151/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7773 - loss: 0.8449 - val_accuracy: 0.0897 - val_loss: 12.0962\n",
      "Epoch 152/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7831 - loss: 0.8237 - val_accuracy: 0.0896 - val_loss: 12.1587\n",
      "Epoch 153/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7800 - loss: 0.8326 - val_accuracy: 0.0897 - val_loss: 12.1787\n",
      "Epoch 154/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7806 - loss: 0.8345 - val_accuracy: 0.0886 - val_loss: 12.1832\n",
      "Epoch 155/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7811 - loss: 0.8276 - val_accuracy: 0.0897 - val_loss: 12.2028\n",
      "Epoch 156/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7813 - loss: 0.8275 - val_accuracy: 0.0903 - val_loss: 12.1693\n",
      "Epoch 157/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7822 - loss: 0.8231 - val_accuracy: 0.0888 - val_loss: 12.2303\n",
      "Epoch 158/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7709 - loss: 0.8639 - val_accuracy: 0.0883 - val_loss: 12.2439\n",
      "Epoch 159/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7866 - loss: 0.8077 - val_accuracy: 0.0886 - val_loss: 12.2549\n",
      "Epoch 160/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7817 - loss: 0.8230 - val_accuracy: 0.0892 - val_loss: 12.2532\n",
      "Epoch 161/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7841 - loss: 0.8159 - val_accuracy: 0.0899 - val_loss: 12.2776\n",
      "Epoch 162/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7845 - loss: 0.8142 - val_accuracy: 0.0879 - val_loss: 12.3230\n",
      "Epoch 163/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7834 - loss: 0.8161 - val_accuracy: 0.0908 - val_loss: 12.3008\n",
      "Epoch 164/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7860 - loss: 0.8095 - val_accuracy: 0.0895 - val_loss: 12.3302\n",
      "Epoch 165/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7858 - loss: 0.8065 - val_accuracy: 0.0864 - val_loss: 12.3707\n",
      "Epoch 166/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7846 - loss: 0.8109 - val_accuracy: 0.0897 - val_loss: 12.3381\n",
      "Epoch 167/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7848 - loss: 0.8087 - val_accuracy: 0.0893 - val_loss: 12.3589\n",
      "Epoch 168/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7856 - loss: 0.8064 - val_accuracy: 0.0879 - val_loss: 12.3914\n",
      "Epoch 169/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7851 - loss: 0.8106 - val_accuracy: 0.0895 - val_loss: 12.4157\n",
      "Epoch 170/170\n",
      "\u001b[1m2050/2050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7860 - loss: 0.8036 - val_accuracy: 0.0885 - val_loss: 12.3903\n",
      "\n",
      "--- Geração com GRU ---\n",
      "Semente: \"harry potter olhou para o castelo e\" | Temperatura: 0.7\n",
      "Texto gerado:\n",
      "------------------\n",
      "harry potter olhou para o castelo e quase caiu do banco do castelo onze a londres não conseguia lembrar do tipo de mundo muito tipo de mundo quanto mais tivesse tempo desde que nunca ouvira embora não podia deixar sabia como exatamente como será que ele sabia \n",
      "------------------\n",
      "\n",
      "Modelo GRU salvo como 'modelo_gru_harry_potter.keras'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Modelo 3: GRU ---\n",
    "print(\"\\n### Treinando o Modelo GRU ###\")\n",
    "gru_model = create_model(GRU, vocab_size, seq_length)\n",
    "gru_model.fit(X, y, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1)\n",
    "\n",
    "print(\"\\n--- Geração com GRU ---\")\n",
    "generate_text(gru_model, tokenizer, seed)\n",
    "\n",
    "# Salvar o modelo\n",
    "gru_model.save('modelo_gru_harry_potter.keras')\n",
    "print(\"Modelo GRU salvo como 'modelo_gru_harry_potter.keras'\")\n",
    "\n",
    "del gru_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab583ad",
   "metadata": {},
   "source": [
    "### 6. Carregar Modelos Salvos e Gerar Novos Textos\n",
    "\n",
    "Use esta seção para carregar qualquer um dos modelos treinados e gerar texto com diferentes sementes (frases iniciais)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "938aba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardopn/Documents/Git/UFSM/rnn-lstm-presentation/.venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 20 variables whereas the saved optimizer has 24 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Geração com SimpleRNN Carregado ---\n",
      "Semente: \"Era uma vez um menino\" | Temperatura: 0.7\n",
      "Texto gerado:\n",
      "------------------\n",
      "Era uma vez um menino de lareira atirou os dois garotos viu os cabelos de gilderoy lockhart arre estava sentada na parede maciça formada louca às suas costas colin franziu certa dificuldade com a popular \n",
      "------------------\n",
      "\n",
      "\n",
      "--- Geração com LSTM Carregado ---\n",
      "Semente: \"Era uma vez um menino\" | Temperatura: 0.7\n",
      "Texto gerado:\n",
      "------------------\n",
      "Era uma vez um menino de aspecto severo quanto lord de perguntas que estava crescendo ninguém podia perceber o que havia à cara livre e o truque afinal achou que não fizessem bruxarias como passar \n",
      "------------------\n",
      "\n",
      "\n",
      "--- Geração com GRU Carregado ---\n",
      "Semente: \"Era uma vez um menino\" | Temperatura: 0.7\n",
      "Texto gerado:\n",
      "------------------\n",
      "Era uma vez um menino muito boa e cinco em cinco metros era uma brincadeira cinco minutos em casa será a sua mulher sangue ruim ultimamente e sempre resmungando velho ali fora da sra dursley \n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escolha qual modelo carregar (descomente a linha desejada)\n",
    "lstm_model = 'modelo_lstm_harry_potter.keras'  # Altere para o modelo desejado\n",
    "simplernn_model = 'modelo_simplernn_harry_potter.keras'\n",
    "gru_model = 'modelo_gru_harry_potter.keras'\n",
    "\n",
    "loaded_lstm_model = tf.keras.models.load_model(lstm_model)\n",
    "loaded_simplernn_model = tf.keras.models.load_model(simplernn_model)\n",
    "loaded_gru_model = tf.keras.models.load_model(gru_model)\n",
    "\n",
    "# Defina sua própria frase inicial aqui\n",
    "nova_semente = \"Era uma vez um menino\"\n",
    "num_words_to_gen=30\n",
    "temperature=0.7\n",
    "\n",
    "# Gerar texto com o modelo carregado\n",
    "# Você pode ajustar a temperatura para controlar a criatividade:\n",
    "# - temperature=0.5 : Mais conservador e previsível\n",
    "# - temperature=0.7 : Balanceado (padrão)\n",
    "# - temperature=1.0 : Mais criativo e arriscado\n",
    "print(\"\\n--- Geração com SimpleRNN Carregado ---\")\n",
    "generate_text(loaded_simplernn_model, tokenizer, nova_semente, num_words_to_gen=num_words_to_gen, temperature=temperature)\n",
    "\n",
    "print(\"\\n--- Geração com LSTM Carregado ---\")\n",
    "generate_text(loaded_lstm_model, tokenizer, nova_semente, num_words_to_gen=num_words_to_gen, temperature=temperature)\n",
    "\n",
    "print(\"\\n--- Geração com GRU Carregado ---\")\n",
    "generate_text(loaded_gru_model, tokenizer, nova_semente, num_words_to_gen=num_words_to_gen, temperature=temperature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn-lstm-presentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
