{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da3baca",
   "metadata": {},
   "source": [
    "# Problema 4 - Geração de Texto com RNN, LSTM e GRU\n",
    "\n",
    "Este notebook tem como objetivo comparar o desempenho de três arquiteturas de redes neurais recorrentes (`SimpleRNN`, `LSTM` e `GRU`) em uma tarefa de **geração de texto caractere por caractere**.\n",
    "\n",
    "## Objetivo\n",
    "- Treinar os três modelos para aprender a estrutura da linguagem a partir dos livros da série Harry Potter.\n",
    "- Gerar texto com cada um dos modelos para comparar visualmente a sua capacidade de criar palavras e frases coerentes.\n",
    "- Discutir as diferenças de performance e eficiência entre `SimpleRNN`, `LSTM` e `GRU`.\n",
    "\n",
    "## Dataset\n",
    "Utilizaremos os três primeiros livros da série Harry Potter em português. Os modelos aprenderão a prever o próximo caractere de uma sequência, e usaremos esse poder preditivo para gerar novos textos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e7d14",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configurar seed para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77894d3",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Preparação dos Dados\n",
    "\n",
    "Vamos carregar os textos dos 3 livros, juntá-los em um único corpus e criar o nosso vocabulário de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e concatenar os textos\n",
    "text = \"\"\n",
    "for i in range(1, 4):\n",
    "    filepath = os.path.join('dataset', f'harry_potter_{i}.txt')\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text += f.read()\n",
    "\n",
    "# Converter para minúsculas para reduzir o vocabulário\n",
    "text = text.lower()\n",
    "\n",
    "# Criar o vocabulário de caracteres\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Tamanho do corpus: {len(text)} caracteres\")\n",
    "print(f\"Tamanho do vocabulário: {vocab_size} caracteres\")\n",
    "print(f\"Vocabulário: {''.join(chars)}\")\n",
    "\n",
    "# Criar dicionários de mapeamento\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f5003",
   "metadata": {},
   "source": [
    "## 3. Criação das Sequências de Treino\n",
    "\n",
    "Agora, vamos transformar nosso longo texto em pares de `(entrada, saída)` para treinar os modelos. Usaremos uma janela deslizante para criar as sequências.\n",
    "\n",
    "- **Tamanho da Sequência (`seq_length`):** 100 caracteres.\n",
    "- **Entrada (`X`):** Uma sequência de 100 caracteres.\n",
    "- **Saída (`y`):** O 101º caractere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(text) - seq_length, 1):\n",
    "    # Sequência de entrada\n",
    "    in_seq = text[i:i + seq_length]\n",
    "    # Caractere de saída\n",
    "    out_char = text[i + seq_length]\n",
    "    # Armazenar como inteiros\n",
    "    X_data.append([char_to_int[char] for char in in_seq])\n",
    "    y_data.append(char_to_int[out_char])\n",
    "\n",
    "n_patterns = len(X_data)\n",
    "print(f\"Total de sequências de treino: {n_patterns}\")\n",
    "\n",
    "# Preparar os dados para a rede neural\n",
    "X = np.reshape(X_data, (n_patterns, seq_length, 1))\n",
    "# Normalizar\n",
    "X = X / float(vocab_size)\n",
    "# One-hot encode da variável de saída\n",
    "y = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83393db9",
   "metadata": {},
   "source": [
    "## 4. Construção e Treinamento dos Modelos\n",
    "\n",
    "Vamos criar uma função para construir os modelos, pois a arquitetura será muito semelhante, mudando apenas a camada recorrente. Em seguida, treinaremos cada um deles.\n",
    "\n",
    "**Atenção:** O treinamento pode ser demorado. Para uma demonstração, poucas épocas já são suficientes para ver a diferença de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import gc\n",
    "\n",
    "def create_model(recurrent_layer, vocab_size, seq_length):\n",
    "    model = Sequential([\n",
    "        recurrent_layer(128, input_shape=(seq_length, 1), return_sequences=True),\n",
    "        recurrent_layer(128),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_text(model, seed_text, num_chars_to_gen=200):\n",
    "    # Converter a semente para minúsculas\n",
    "    full_generated_text = seed_text.lower()\n",
    "    \n",
    "    # Preparar a semente inicial para ter o tamanho correto (seq_length)\n",
    "    # Se a semente for maior, pegamos os últimos 'seq_length' caracteres.\n",
    "    # Se for menor, preenchemos com espaços à esquerda.\n",
    "    current_pattern_text = full_generated_text.rjust(seq_length)\n",
    "    \n",
    "    # Converter o padrão inicial para inteiros\n",
    "    pattern = [char_to_int[char] for char in current_pattern_text[-seq_length:]]\n",
    "    \n",
    "    print(f\"Semente: \\\"{seed_text}\\\"\")\n",
    "    print(\"Texto gerado:\")\n",
    "    print(\"------------------\")\n",
    "    print(seed_text, end='') # Imprime a semente original\n",
    "    \n",
    "    for i in range(num_chars_to_gen):\n",
    "        # Preparar entrada para o modelo (sempre com seq_length)\n",
    "        x = np.reshape(pattern, (1, seq_length, 1))\n",
    "        x = x / float(vocab_size)\n",
    "        \n",
    "        # Fazer a previsão\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Pegar o caractere com a maior probabilidade\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        \n",
    "        # Adicionar ao texto gerado e atualizar o padrão\n",
    "        full_generated_text += result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)] # Mantém o padrão com o tamanho de seq_length\n",
    "        \n",
    "        # Imprimir o caractere gerado\n",
    "        print(result, end='')\n",
    "        \n",
    "    print(\"\\n------------------\\n\")\n",
    "\n",
    "# Configurar callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',          # Métrica a monitorar\n",
    "    patience=2,                  # Parar após 2 épocas sem melhoria (era 3)\n",
    "    min_delta=0.001,             # Considerar melhora apenas se for > 0.001\n",
    "    restore_best_weights=True,   # Restaurar os melhores pesos\n",
    "    verbose=1                    # Mostrar quando parar\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',          # Métrica a monitorar\n",
    "    factor=0.5,                  # Reduzir LR pela metade\n",
    "    patience=1,                  # Reduzir LR após 1 época sem melhora (era 2)\n",
    "    min_lr=1e-6,                 # LR mínimo\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "epochs = 20\n",
    "seed = \"harry potter olhou para o castelo e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ac7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modelo 1: SimpleRNN ---\n",
    "print(\"### Treinando o Modelo SimpleRNN ###\")\n",
    "rnn_model = create_model(SimpleRNN, vocab_size, seq_length)\n",
    "rnn_model.fit(X, y, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1, callbacks=callbacks)\n",
    "\n",
    "print(\"\\n--- Geração com SimpleRNN ---\")\n",
    "generate_text(rnn_model, seed)\n",
    "\n",
    "# Limpar memória\n",
    "del rnn_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modelo 2: LSTM ---\n",
    "print(\"\\n### Treinando o Modelo LSTM ###\")\n",
    "lstm_model = create_model(LSTM, vocab_size, seq_length)\n",
    "lstm_model.fit(X, y, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1, callbacks=callbacks)\n",
    "\n",
    "print(\"\\n--- Geração com LSTM ---\")\n",
    "generate_text(lstm_model, seed)\n",
    "\n",
    "# Limpar memória\n",
    "del lstm_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e46567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modelo 3: GRU ---\n",
    "print(\"\\n### Treinando o Modelo GRU ###\")\n",
    "gru_model = create_model(GRU, vocab_size, seq_length)\n",
    "gru_model.fit(X, y, epochs=epochs, batch_size=128, verbose=1, validation_split=0.1, callbacks=callbacks)\n",
    "\n",
    "print(\"\\n--- Geração com GRU ---\")\n",
    "generate_text(gru_model, seed)\n",
    "\n",
    "# Limpar memória\n",
    "del gru_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn-lstm-presentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
